{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INM 702 Coursework Code: Task 3\n",
    "## Implementation of Softmax classifier\n",
    "### By: Jasveen Kaur and Nikhil Vallakati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary packages for matrix computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a softmax classifier function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_basic(z):\n",
    "        exps = np.exp(z)\n",
    "        sums = np.sum(exps)\n",
    "        return np.divide(exps, sums)\n",
    "\n",
    "    \n",
    "def softmax_grad(s): \n",
    "    #a = np.diag(np.diag(s))\n",
    "    S_vector = np.diag(s)\n",
    "    S_matrix = np.transpose(S_vector)\n",
    "    return np.diag(s) - (S_matrix * np.transpose(S_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a sample input array (6x3) along with its label (6x1) (same as task 1 and task 2) to implement forward and backward pass:\n",
    "2 more input sets were defined, one with higher input values and the other with high negative input value, to examine the numerical computation problem that softmax classifier undergoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_set = np.array([[0,1,0],\n",
    "                      [0,0,1],\n",
    "                      [1,0,0],\n",
    "                      [1,1,0],\n",
    "                      [1,1,1],\n",
    "                      [0,1,1],\n",
    "                     ])#Dependent variable\n",
    "input_set2 = np.array([[-2000,-6000,-2000],\n",
    "                      [-2000,-2000,-6000],\n",
    "                      [-6000,-2000,-2000],\n",
    "                      [-6000,-6000,-2000],\n",
    "                      [-6000,-6000,-6000],\n",
    "                      [-2000,-6000,-6000],\n",
    "                     ])#Dependent variable\n",
    "input_set3 = np.array([[2000,6000,2000],\n",
    "                      [2000,2000,6000],\n",
    "                      [6000,2000,2000],\n",
    "                      [6000,6000,2000],\n",
    "                      [6000,6000,6000],\n",
    "                      [2000,6000,6000],\n",
    "                     ])#Dependent variable\n",
    "labels = np.array([[1,\n",
    "                    0,\n",
    "                    0,\n",
    "                    1,\n",
    "                    1,\n",
    "                    0,]])\n",
    "labels = labels.reshape(6,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward and backward pass on the above defined array, with Relu on input layer and softmax classifier on the output layer to calculate probabilities and backpropagate through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network(object):\n",
    "\n",
    "    def __init__(self, n_hidden, epochs, lr, seed):\n",
    "\n",
    "        self.random = np.random.RandomState(seed)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.seed = seed\n",
    "    \n",
    "    #relu activation function\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x) \n",
    "    \n",
    "    #derivative of relu activation function(element)\n",
    "    def relu_d_element(self, x):\n",
    "        if x > 0:\n",
    "            return 1\n",
    "        elif x <= 0:\n",
    "            return 0\n",
    "    \n",
    "    #derivative of relu activation function(array)\n",
    "    def relu_d_array(self, x):\n",
    "        x[x<=0] = 0\n",
    "        x[x>0] = 1\n",
    "        return x\n",
    "        \n",
    "    #softmax function definition   \n",
    "    def softmax_basic(self, x):\n",
    "        exp_scores = np.exp(x)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        return probs\n",
    "\n",
    "    #softmax to calculate gradient\n",
    "    def softmax_grad(self, s): \n",
    "        #a = np.diag(np.diag(s))\n",
    "        S_vector = np.diag(s)\n",
    "        S_matrix = np.transpose(S_vector)\n",
    "        return np.diag(s) - (S_matrix * np.transpose(S_matrix))\n",
    "    \n",
    "    #forward pass\n",
    "    def forward_pass(self, X):\n",
    "      \n",
    "        z1 = np.dot(X, self.w1) + self.b1\n",
    "        a1 = self.relu(z1)\n",
    "        \n",
    "        z_out = np.dot(a1, self.w_out) + self.b_out\n",
    "        a_out = self.softmax_basic(z_out)\n",
    "\n",
    "        return z1, a1, z_out, a_out   \n",
    "\n",
    "    #computing the loss term\n",
    "    def compute_cost(self, y_enc, a_out):\n",
    "        \n",
    "        #Normal loss function   \n",
    "        #term1 = a_out - y_enc \n",
    "        #cost = term1.sum()\n",
    "        #return cost\n",
    "    \n",
    "        #Cross Entropy Loss\n",
    "        y_enc = y_enc.argmax(axis=1)\n",
    "        m = y_enc.shape[0]\n",
    "        log_probs = -np.log(a_out[range(m),y_enc])\n",
    "        loss = np.sum(log_probs)/m\n",
    "        return loss\n",
    "    \n",
    "    #predicting the output\n",
    "    def predict_out(self, X):\n",
    "        z1, a1,z_out, a_out = self.forward_pass(X)\n",
    "        y_pred = np.argmax(a_out, axis=1)\n",
    "        return y_pred\n",
    "    \n",
    "    #calculating the accuracy \n",
    "    def accuracy(self, y, y_pred, X):\n",
    "        return ((np.sum(y.T == y_pred)).astype(np.float) / X.shape[0])\n",
    "    \n",
    "    #Training the network\n",
    "    def train(self, X_train, y_train):\n",
    "        \n",
    "        n_output = np.unique(y_train).shape[0]\n",
    "        n_features = X_train.shape[1]\n",
    "\n",
    "        #Initializing the weights\n",
    "        \n",
    "        #hidden layer\n",
    "        self.b1 = np.zeros(self.n_hidden)\n",
    "        self.w1 = self.random.normal(loc=0.0, scale=0.1,size=(n_features, self.n_hidden))\n",
    "        \n",
    "        #output layer\n",
    "        self.b_out = np.zeros(n_output)\n",
    "        self.w_out = self.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden, n_output))\n",
    "        \n",
    "        self.results = {'cost': [], 'train_acc': []}  \n",
    "        \n",
    "        #training epochs\n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            #forward propagation\n",
    "            z1, a1, z_out, a_out = self.forward_pass(X_train)\n",
    "            #data_cost = self.compute_cost(y_train, a_out)\n",
    "            \n",
    "            # Backpropagation\n",
    "            \n",
    "            sigma_out = a_out - labels #sigma_out = error\n",
    "            relu_derivative = self.relu_d_array(a1)\n",
    "            sigma_h1 = (np.dot(sigma_out, self.w_out.T) * relu_derivative)\n",
    "            \n",
    "            grad_w1 = np.dot(X_train.T, sigma_h1)\n",
    "            grad_b1 = np.sum(sigma_h1, axis=0)\n",
    "\n",
    "            grad_w_out = np.dot(a1.T, sigma_out)\n",
    "            grad_b_out = np.sum(sigma_out, axis=0)\n",
    "\n",
    "            delta_w1 = grad_w1\n",
    "            delta_w_out = grad_w_out  \n",
    "\n",
    "            delta_b1 = grad_b1\n",
    "            delta_b_out = grad_b_out\n",
    "                \n",
    "             #updating the weights\n",
    "            self.w1 -= self.lr * delta_w1\n",
    "            self.w_out -= self.lr * delta_w_out\n",
    "\n",
    "            self.b1 -= self.lr * delta_b1           \n",
    "            self.b_out -= self.lr * delta_b_out\n",
    "            \n",
    "            \n",
    "            #evaluating the trained model with updated weights\n",
    "            z1, a1, z_out, a_out = self.forward_pass(X_train)\n",
    "            \n",
    "            cost = self.compute_cost(y_enc=labels, a_out=a_out)\n",
    "            y_train_pred = self.predict_out(X_train)\n",
    "\n",
    "            train_acc = self.accuracy(y_train, y_train_pred, X_train)\n",
    "            \n",
    "            print(\"epoch:\", i+1)\n",
    "            print(\"Accuracy:\",\"{:.2f}\".format(train_acc*100),\"% ||\",\"loss:\",\"{:.3f}\".format(cost))\n",
    "            \n",
    "            self.results['cost'].append(cost)\n",
    "            self.results['train_acc'].append(train_acc)\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the parameters and propagating through one layer network for input_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 2\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 3\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 4\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 5\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 6\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 7\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 8\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 9\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 10\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 11\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 12\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 13\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 14\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 15\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 16\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 17\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 18\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 19\n",
      "Accuracy: 66.67 % || loss: 0.693\n",
      "epoch: 20\n",
      "Accuracy: 66.67 % || loss: 0.693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.neural_network at 0x21c89585860>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model = neural_network(n_hidden=7, epochs=20, lr=0.0001, seed=1)\n",
    "Model.train(X_train=input_set, y_train=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the parameters and propagating through one layer network for input_set2 (High negative values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Accuracy: 50.00 % || loss: 95.002\n",
      "epoch: 2\n",
      "Accuracy: 50.00 % || loss: 27.516\n",
      "epoch: 3\n",
      "Accuracy: 50.00 % || loss: 14.339\n",
      "epoch: 4\n",
      "Accuracy: 50.00 % || loss: 29.510\n",
      "epoch: 5\n",
      "Accuracy: 66.67 % || loss: 14.899\n",
      "epoch: 6\n",
      "Accuracy: 50.00 % || loss: 36.413\n",
      "epoch: 7\n",
      "Accuracy: 50.00 % || loss: 22.890\n",
      "epoch: 8\n",
      "Accuracy: 50.00 % || loss: 33.285\n",
      "epoch: 9\n",
      "Accuracy: 50.00 % || loss: 37.685\n",
      "epoch: 10\n",
      "Accuracy: 50.00 % || loss: 27.232\n",
      "epoch: 11\n",
      "Accuracy: 50.00 % || loss: 32.071\n",
      "epoch: 12\n",
      "Accuracy: 33.33 % || loss: 25.222\n",
      "epoch: 13\n",
      "Accuracy: 50.00 % || loss: 21.154\n",
      "epoch: 14\n",
      "Accuracy: 50.00 % || loss: 34.921\n",
      "epoch: 15\n",
      "Accuracy: 66.67 % || loss: 19.297\n",
      "epoch: 16\n",
      "Accuracy: 50.00 % || loss: 54.204\n",
      "epoch: 17\n",
      "Accuracy: 50.00 % || loss: 27.830\n",
      "epoch: 18\n",
      "Accuracy: 50.00 % || loss: 54.412\n",
      "epoch: 19\n",
      "Accuracy: 50.00 % || loss: 28.096\n",
      "epoch: 20\n",
      "Accuracy: 66.67 % || loss: 45.938\n",
      "epoch: 21\n",
      "Accuracy: 50.00 % || loss: 35.901\n",
      "epoch: 22\n",
      "Accuracy: 66.67 % || loss: 50.895\n",
      "epoch: 23\n",
      "Accuracy: 50.00 % || loss: 37.116\n",
      "epoch: 24\n",
      "Accuracy: 66.67 % || loss: 54.906\n",
      "epoch: 25\n",
      "Accuracy: 66.67 % || loss: 37.663\n",
      "epoch: 26\n",
      "Accuracy: 66.67 % || loss: nan\n",
      "epoch: 27\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 28\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 29\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 30\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 31\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 32\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 33\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 34\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 35\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 36\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 37\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 38\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 39\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 40\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 41\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 42\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 43\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 44\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 45\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 46\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 47\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 48\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 49\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 50\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 51\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 52\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 53\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 54\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 55\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 56\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 57\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 58\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 59\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 60\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 61\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 62\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 63\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 64\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 65\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 66\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 67\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 68\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 69\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 70\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 71\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 72\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 73\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 74\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 75\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 76\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 77\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 78\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 79\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 80\n",
      "Accuracy: 50.00 % || loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3.7\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\asus\\anaconda3.7\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in less_equal\n",
      "C:\\Users\\asus\\anaconda3.7\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.neural_network at 0x21c896559e8>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model = neural_network(n_hidden=7, epochs=80, lr=0.0001, seed=1)\n",
    "Model.train(X_train=input_set2, y_train=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the parameters and propagating through one layer network for input_set3 (High positive values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Accuracy: 66.67 % || loss: 3.117\n",
      "epoch: 2\n",
      "Accuracy: 50.00 % || loss: 2.768\n",
      "epoch: 3\n",
      "Accuracy: 50.00 % || loss: 0.000\n",
      "epoch: 4\n",
      "Accuracy: 33.33 % || loss: 5.725\n",
      "epoch: 5\n",
      "Accuracy: 50.00 % || loss: 0.074\n",
      "epoch: 6\n",
      "Accuracy: 16.67 % || loss: 9.015\n",
      "epoch: 7\n",
      "Accuracy: 16.67 % || loss: 6.125\n",
      "epoch: 8\n",
      "Accuracy: 16.67 % || loss: 5.635\n",
      "epoch: 9\n",
      "Accuracy: 16.67 % || loss: 5.149\n",
      "epoch: 10\n",
      "Accuracy: 33.33 % || loss: 3.982\n",
      "epoch: 11\n",
      "Accuracy: 16.67 % || loss: 12.453\n",
      "epoch: 12\n",
      "Accuracy: 16.67 % || loss: 12.000\n",
      "epoch: 13\n",
      "Accuracy: 16.67 % || loss: nan\n",
      "epoch: 14\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 15\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 16\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 17\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 18\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 19\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 20\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 21\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 22\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 23\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 24\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 25\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 26\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 27\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 28\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 29\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 30\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 31\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 32\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 33\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 34\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 35\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 36\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 37\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 38\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 39\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 40\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 41\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 42\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 43\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 44\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 45\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 46\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 47\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 48\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 49\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 50\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 51\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 52\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 53\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 54\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 55\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 56\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 57\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 58\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 59\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 60\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 61\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 62\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 63\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 64\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 65\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 66\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 67\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 68\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 69\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 70\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 71\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 72\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 73\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 74\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 75\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 76\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 77\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 78\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 79\n",
      "Accuracy: 50.00 % || loss: nan\n",
      "epoch: 80\n",
      "Accuracy: 50.00 % || loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3.7\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\asus\\anaconda3.7\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in less_equal\n",
      "C:\\Users\\asus\\anaconda3.7\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.neural_network at 0x21c89655da0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model = neural_network(n_hidden=7, epochs=80, lr=0.0001, seed=1)\n",
    "Model.train(X_train=input_set3, y_train=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy obtained by softmax classifier: It gave reasonable results with the first input matrix (input_set). However, the other two matrices show that if we have too high and too low input values, then the loss function faces issues with numerical computation (as described in the report) and produces a \"nan\" loss value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
